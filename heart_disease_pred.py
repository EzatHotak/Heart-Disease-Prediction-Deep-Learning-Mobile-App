# -*- coding: utf-8 -*-
"""heart_disease_pred.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qy72MM5hmqmIo-qLb5Gk0QakWMCxaRxo

# Libraries of Pythong and languges used for AI (Machine Learning Diagnostic App)
* Pandas: for Data Analysis.
* Numpy: for numerical operations.
* Matplotlib: for Data Visualization.
* Scikit-learn for spliting data into Training and Test sets
* Seaborn: for Data Visualization.
* Tensorflow: for Modeling.
* Tensorflow Lite: for converting our trained Model to working model on Mobile and API Handling.
* Flutter for User-Interface, taking Input, and presenting output to the User.
"""

import numpy as np # np is short for numpy

import pandas as pd # pandas is so commonly used, it's shortened to pd

import matplotlib
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

import seaborn as sns

import tensorflow as tf

df = pd.read_csv("/content/sample_data/heart_disease.csv")
df.shape

"""### 1.4 Which features of the data will be important to us?

Features are different parts and characteristics of the data.

During this step, you'll want to start exploring what each portion of the data relates to and then create a reference you can use to look up later on.

One of the most common ways to do this is to create a **data dictionary**.

#### Heart Disease Data Dictionary

A data dictionary describes the data you're dealing with.

Not all datasets come with them so this is where you may have to do your research or ask a **subject matter expert** (someone who knows about the data) for more.

The following are the features we'll use to predict our target variable (heart disease or no heart disease).

| Feature  | Description | Example Values |
|:-----|:-----|:------|
| **age** | Age in years | 29, 45, 60 |
| **sex** | 1 = male; 0 = female | 0, 1  |
| **cp**  | Chest pain type | 0: Typical angina (chest pain), 1: Atypical angina (chest pain not related to heart), 2: Non-anginal pain (typically esophageal spasms (non heart related), 3: Asymptomatic (chest pain not showing signs of disease) |
| **trestbps** | Resting blood pressure (in mm Hg on admission to the hospital)  | 120, 140, 150 |
| **chol** | Serum cholesterol in mg/dl | 180, 220, 250 |
| **fbs** | Fasting blood sugar > 120 mg/dl (1 = true; 0 = false) | 0, 1 |
| **restecg** | Resting electrocardiographic results | 0: Nothing to note, 1: ST-T Wave abnormality, 2: Left ventricular hypertrophy  |
| **thalach** | Maximum heart rate achieved | 160, 180, 190 |
| **exang**  | Exercise induced angina (1 = yes; 0 = no) | 0, 1 |
| **oldpeak**  | ST depression (heart potentially not getting enough oxygen) induced by exercise relative to rest | 0.5, 1.0, 2.0  |
| **slope** | The slope of the peak exercise ST segment | 0: Upsloping, 1: Flatsloping, 2: Downsloping |
| **ca** | Number of major vessels (0-3) colored by fluoroscopy | 0, 1, 2, 3 |
| **thal** | Thalium stress result  | 1: Normal, 3: Normal, 6: Fixed defect, 7: Reversible defect |
| **target** | Have disease or not (1 = yes; 0 = no) | 0, 1 |

> **Note:** No personal identifiable information (PPI) can be found in the dataset.

It's a good idea to save these to a Python dictionary or in an external file, so we can look at them later without coming back here.
"""

df.head()

df.target.value_counts()

# Normalized value counts
print("percetange of people who has/has not heart-disease:",df.target.value_counts(normalize=True)*100)

# Plot the value counts with a bar graph
df.target.value_counts().plot(kind="bar", color=["salmon", "lightblue"]);

df.info()

df.describe()

df.sex.value_counts()

# Compare target column with sex column
pd.crosstab(index=df.target, columns=df.sex)

# Let's visualize it
# Create a plot
pd.crosstab(df.target, df.sex).plot(kind="bar", figsize=(10,6), color=["salmon", "lightblue"])

# Add some attributes to it
plt.title("Heart Disease Frequency vs Sex")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["Female", "Male"])
plt.xticks(rotation=0); # keep the labels on the x-axis vertical

"""### Comparing age and maximum heart rate"""

# Histograms are a great way to check the distribution of a variable
# Create another figure
plt.figure(figsize=(10,6))

# Start with positve examples
plt.scatter(df.age[df.target==1],
            df.thalach[df.target==1],
            c="salmon") # define it as a scatter figure

# Now for negative examples, we want them on the same plot, so we call plt again
plt.scatter(df.age[df.target==0],
            df.thalach[df.target==0],
            c="lightblue") # axis always come as (x, y)

# Add some helpful info
plt.title("Heart Disease in function of Age and Max Heart Rate")
plt.xlabel("Age")
plt.legend(["Disease", "No Disease"])
plt.ylabel("Max Heart Rate");

# Histograms are a great way to check the distribution of a variable
df.age.plot.hist();

# Create a new crosstab and base plot
pd.crosstab(df.cp, df.target).plot(kind="bar",
                                   figsize=(10,6),
                                   color=["lightblue", "salmon"])

# Add attributes to the plot to make it more readable
plt.title("Heart Disease Frequency Per Chest Pain Type")
plt.xlabel("Chest Pain Type")
plt.ylabel("Frequency")
plt.legend(["No Disease", "Disease"])
plt.xticks(rotation = 0);

"""What can we infer from this?

Remember from our data dictionary what the different levels of chest pain are.

| Feature  | Description | Example Values |
|:-----|:-----|:------|
| **cp**  | Chest pain type | 0: Typical angina (chest pain), 1: Atypical angina (chest pain not related to heart), 2: Non-anginal pain (typically esophageal spasms (non heart related), 3: Asymptomatic (chest pain not showing signs of disease) |
    
It's interesting that atypical angina (value 1) states it's not related to the heart but seems to have a higher ratio of participants with heart disease than not.

Wait...?

What does *atypical agina* even mean?

At this point, it's important to remember, if your data dictionary doesn't supply you enough information, you may want to do further research on your values.

This research may come in the form of asking a **subject matter expert** (such as a cardiologist or the person who gave you the data) or Googling to find out more.

According to PubMed, it seems [even some medical professionals are confused by the term](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2763472/).

> Today, 23 years later, â€œatypical chest painâ€ is still popular in medical circles. Its meaning, however, remains unclear. A few articles have the term in their title, but do not define or discuss it in their text. In other articles, the term refers to noncardiac causes of chest pain.

Although not conclusive, the plot above is a sign there may be a confusion of defintions being represented in data.
"""

## Turning our Data into X and y (Features and labels)
# Everything except target variable
X = df.drop(labels="target", axis=1)

# Target variable
y = df.target

"""
### Turning our data into training and Test sets"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Mean of X_train after scaling:\n", X_train.mean())
print("Standard deviation of X_train after scaling:\n", X_train.std())

# Define the model
tf.random.set_seed(42)
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Fit the model
model.fit(X_train, y_train, epochs=70, batch_size=32, validation_data=(X_test, y_test))

sample_input = np.array([
    [41.0, 0.0, 2.0, 130.0, 204.0, 0.0, 0.0, 172.0, 0.0, 1.4, 2.0, 0.0, 1.0],
])
# Run prediction
prediction = model.predict(sample_input)

# Print result
print("Raw prediction (sigmoid output):", prediction)

# Interpretation
if prediction[0][0] >= 0.5:
    print("Prediction: ðŸ”´ Likely has heart disease")
else:
    print("Prediction: ðŸŸ¢ Likely healthy")

model.evaluate(X_test,y_test)

history = model.fit(X_train, y_train,

                    epochs=100,  # Train longer
                    batch_size=32,
                    verbose=1)

# 4. Plotting function for training-only metrics
def plot_training_curves(history):
    plt.figure(figsize=(12, 5))

    # Plot accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], 'b-', label='Training Accuracy')
    plt.title('Training Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend()

    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], 'r-', label='Training Loss')
    plt.title('Training Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()

    plt.tight_layout()
    plt.savefig('training_curves.png')
    plt.show()

# 5. Generate the plot
plot_training_curves(history)

import plotly.graph_objects as go
from plotly.subplots import make_subplots

def plot_training_curves_advanced(history):
    """Plot beautiful interactive training curves with Plotly"""
    # Create subplots
    fig = make_subplots(rows=1, cols=2,
                       subplot_titles=('<b>Model Accuracy</b>', '<b>Model Loss</b>'),
                       horizontal_spacing=0.15)

    # Calculate epochs (fixed the parenthesis issue here)
    epochs = list(range(1, len(history.history['accuracy']) + 1))

    # Add accuracy traces
    fig.add_trace(
        go.Scatter(
            x=epochs,
            y=history.history['accuracy'],
            mode='lines+markers',
            name='Training Accuracy',
            line=dict(color='#1f77b4', width=3),
            marker=dict(size=8),
            hovertemplate='Epoch: %{x}<br>Accuracy: %{y:.3f}'
        ),
        row=1, col=1
    )

    # Add validation accuracy if available
    if 'val_accuracy' in history.history:
        fig.add_trace(
            go.Scatter(
                x=epochs,
                y=history.history['val_accuracy'],
                mode='lines+markers',
                name='Validation Accuracy',
                line=dict(color='#ff7f0e', width=3, dash='dot'),
                marker=dict(size=8),
                hovertemplate='Epoch: %{x}<br>Val Accuracy: %{y:.3f}'
            ),
            row=1, col=1
        )

    # Add loss traces
    fig.add_trace(
        go.Scatter(
            x=epochs,
            y=history.history['loss'],
            mode='lines+markers',
            name='Training Loss',
            line=dict(color='#d62728', width=3),
            marker=dict(size=8),
            hovertemplate='Epoch: %{x}<br>Loss: %{y:.3f}'
        ),
        row=1, col=2
    )

    # Add validation loss if available
    if 'val_loss' in history.history:
        fig.add_trace(
            go.Scatter(
                x=epochs,
                y=history.history['val_loss'],
                mode='lines+markers',
                name='Validation Loss',
                line=dict(color='#2ca02c', width=3, dash='dot'),
                marker=dict(size=8),
                hovertemplate='Epoch: %{x}<br>Val Loss: %{y:.3f}'
            ),
            row=1, col=2
        )

    # Update layout
    fig.update_layout(
        title_text='<b>Training Metrics</b>',
        title_x=0.5,
        title_font=dict(size=24, family='Arial', color='darkblue'),
        showlegend=True,
        hovermode='x unified',
        template='plotly_white',
        width=1000,
        height=500,
        margin=dict(l=50, r=50, b=50, t=100, pad=4),
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        )
    )

    # Update axes
    fig.update_xaxes(title_text='Epoch', row=1, col=1)
    fig.update_yaxes(title_text='Accuracy', row=1, col=1)
    fig.update_xaxes(title_text='Epoch', row=1, col=2)
    fig.update_yaxes(title_text='Loss', row=1, col=2)

    # Save and show

    fig.show()

plot_training_curves_advanced(history)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2

model1 = Sequential([
    # Input layer
    Dense(128, activation='relu', input_shape=(13,), kernel_regularizer=l2(0.01)),
    BatchNormalization(),
    Dropout(0.3),

    # Hidden layers
    Dense(64, activation='relu', kernel_regularizer=l2(0.005)),
    BatchNormalization(),
    Dropout(0.2),

    Dense(32, activation='relu'),
    BatchNormalization(),

    # Output layer
    Dense(1, activation='sigmoid')
])

model1.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy',
                      'Precision',
                      'Recall'])

history = model1.fit(X_train, y_train,

                    epochs=100,  # Train longer
                    batch_size=32,
                    verbose=1)

# 5. Generate the plot
plot_training_curves(history)

plot_training_curves_advanced(history)

sample_input = np.array([
    [41.0, 0.0, 2.0, 130.0, 204.0, 0.0, 0.0, 172.0, 0.0, 1.4, 2.0, 0.0, 1.0],
])
# Run prediction
prediction = model1.predict(sample_input)

# Print result
print("Raw prediction (sigmoid output):", prediction)

# Interpretation
if prediction[0][0] >= 0.5:
    print("Prediction: ðŸ”´ Likely has heart disease")
else:
    print("Prediction: ðŸŸ¢ Likely healthy")

model.evaluate(X_test,y_test)

# Model3 : Modeling Experiments

tf.random.set_seed(42)

# Define the model
tf.random.set_seed(42)
model3 = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='softmax')
])

# Compile the model
model3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Fit the model
model3.fit(X_train, y_train, epochs=70, batch_size=32, validation_data=(X_test, y_test))

history = model3.fit(X_train, y_train,

                    epochs=100,  # Train longer
                    batch_size=32,
                    verbose=1)

# 5. Generate the plot
plot_training_curves(history)

plot_training_curves_advanced(history)

sample_input = np.array([
    [41.0, 0.0, 2.0, 130.0, 204.0, 0.0, 0.0, 172.0, 0.0, 1.4, 2.0, 0.0, 1.0],
])
# Run prediction
prediction = model3.predict(sample_input)

# Print result
print("Raw prediction (sigmoid output):", prediction)

# Interpretation
if prediction[0][0] >= 0.5:
    print("Prediction: ðŸ”´ Likely has heart disease")
else:
    print("Prediction: ðŸŸ¢ Likely healthy")

model3.evaluate(X_test,y_test)

converter=tf.lite.TFLiteConverter.from_keras_model(model1)
tfmodel=converter.convert()
open("heart_disease_pred.tflite","wb",).write(tfmodel)

